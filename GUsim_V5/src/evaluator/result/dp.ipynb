{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&66.1& 24.1& 9.8\n",
      "&47.3& 46.0& 6.8\n",
      "&67.5& 20.4& 12.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 输入数据\n",
    "data1 = \"\"\"\n",
    "Understandability Scores:\n",
    "  Score 1: 63.1\n",
    "  Score 0: 25.5\n",
    "  Score -1: 11.4\n",
    "\n",
    "Naturalness Scores:\n",
    "  Score 1: 60.4\n",
    "  Score 0: 36.2\n",
    "  Score -1: 3.4\n",
    "\n",
    "Recommendation Scores:\n",
    "  Score 1: 73.2\n",
    "  Score 0: 17.4\n",
    "  Score -1: 9.4\n",
    "\"\"\"\n",
    "\n",
    "data2 = \"\"\"\n",
    "Understandability Scores:\n",
    "  Score 1: 8.1\n",
    "  Score 0: 22.8\n",
    "  Score -1: 69.1\n",
    "\n",
    "Naturalness Scores:\n",
    "  Score 1: 10.1\n",
    "  Score 0: 55.7\n",
    "  Score -1: 34.2\n",
    "\n",
    "Recommendation Scores:\n",
    "  Score 1: 14.8\n",
    "  Score 0: 23.5\n",
    "  Score -1: 61.7\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 分别提取各项指标\n",
    "def extract_block(data, section_name):\n",
    "    pattern = rf\"{section_name} Scores:\\n(?:\\s*Score [10-]+: [0-9.]+\\n?)+\"\n",
    "    match = re.search(pattern, data)\n",
    "    if match:\n",
    "        return extract_scores(match.group())\n",
    "    else:\n",
    "        raise ValueError(f\"Section '{section_name}' not found.\")\n",
    "\n",
    "understandability1 = extract_block(data1, \"Understandability\")\n",
    "naturalness1 = extract_block(data1, \"Naturalness\")\n",
    "recommendation1 = extract_block(data1, \"Recommendation\")\n",
    "\n",
    "understandability2 = extract_block(data2, \"Understandability\")\n",
    "naturalness2 = extract_block(data2, \"Naturalness\")\n",
    "recommendation2 = extract_block(data2, \"Recommendation\")\n",
    "\n",
    "\n",
    "# 按照你的规则计算 data3\n",
    "def compute_data3(scores1, scores2):\n",
    "    return {\n",
    "        1: (scores1[1] + scores2[-1]) / 2,   \n",
    "        0: (scores1[0] + scores2[0]) / 2,    \n",
    "        -1: (scores1[-1] + scores2[1]) / 2   \n",
    "    }\n",
    "\n",
    "# 计算三个指标\n",
    "understandability3 = compute_data3(understandability1, understandability2)\n",
    "naturalness3 = compute_data3(naturalness1, naturalness2)\n",
    "recommendation3 = compute_data3(recommendation1, recommendation2)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"&{understandability3[1]:.1f}& {understandability3[0]:.1f}& {understandability3[-1]:.1f}\")\n",
    "print(f\"&{naturalness3[1]:.1f}& {naturalness3[0]:.1f}& {naturalness3[-1]:.1f}\")\n",
    "print(f\"&{recommendation3[1]:.1f}& {recommendation3[0]:.1f}& {recommendation3[-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dialogues analyzed: 142\n",
      "\n",
      "Score distribution:\n",
      "--------------------------------------------------\n",
      "Metric               | -1         | 0          | 1         \n",
      "--------------------------------------------------\n",
      "Naturalness Scores:\n",
      "  Score -1: 54.9\n",
      "  Score 0: 40.1\n",
      "  Score 1: 4.9\n",
      "\n",
      "Recommendation Scores:\n",
      "  Score -1: 76.1\n",
      "  Score 0: 11.3\n",
      "  Score 1: 12.7\n",
      "\n",
      "Understandability Scores:\n",
      "  Score -1: 78.2\n",
      "  Score 0: 11.3\n",
      "  Score 1: 10.6\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_dialogue_scores():\n",
    "    metrics = ['naturalness', 'recommendation', 'understandability']\n",
    "    scores = {metric: {'-1': 0, '0': 0, '1': 0} for metric in metrics}\n",
    "    \n",
    "    # Regular expression pattern to match dialogue scores\n",
    "    import re\n",
    "    pattern = r\"Dialogue \\d+ Scores: \\{'naturalness': '(-?\\d+)', 'recommendation': '(-?\\d+)', 'understandability': '(-?\\d+)'\\}\"\n",
    "    \n",
    "    # Read the file content (assuming it's already loaded in a string variable 'content')\n",
    "    with open('/data/liyuanzi/HUAWEI/GUsim_V5/src/evaluator/result/eval_common_pairwiseB4ominiVSB3.5_U38B2.log', 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, content)\n",
    "    \n",
    "    # Count the occurrences\n",
    "    total_dialogues = len(matches)\n",
    "    for match in matches:\n",
    "        naturalness, recommendation, understandability = match\n",
    "        scores['naturalness'][naturalness] += 1\n",
    "        scores['recommendation'][recommendation] += 1\n",
    "        scores['understandability'][understandability] += 1\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Total dialogues analyzed: {total_dialogues}\")\n",
    "    print(\"\\nScore distribution:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Metric':<20} | {'-1':<10} | {'0':<10} | {'1':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.capitalize()} Scores:\")\n",
    "        for score, count in scores[metric].items():\n",
    "            print(f\"  Score {score}: {count / total_dialogues * 100:.1f}\")\n",
    "        print()\n",
    "        minus_one = scores[metric]['-1']\n",
    "        zero = scores[metric]['0']\n",
    "        one = scores[metric]['1']\n",
    "        minus_one_percent = (minus_one / total_dialogues) * 100\n",
    "        zero_percent = (zero / total_dialogues) * 100\n",
    "        one_percent = (one / total_dialogues) * 100\n",
    "        \n",
    "        # print(f\"{metric:<20} | {minus_one:<5} ({minus_one_percent:.1f}%) | {zero:<5} ({zero_percent:.1f}%) | {one:<5} ({one_percent:.1f}%)\")\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "analyze_dialogue_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Usim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
